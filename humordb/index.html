<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> HumorDB | Vedaant Jain </title> <meta name="author" content="Vedaant Jain"> <meta name="description" content="Researcher | Architecting Full-Stack Reliable AI Systems "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vedaant-j.github.io/humordb/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Vedaant Jain </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research/Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">HumorDB</h1> <p class="post-description"></p> </header> <article> <h2 id="humordb-can-ai-understand-graphical-humor">HumorDB: Can AI Understand Graphical Humor?</h2> <p><strong>Authors:</strong> <a href="/">Vedaant V Jain</a>, Felipe dos Santos Alves Feitosa, Gabriel Kreiman <br> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025</em></p> <div class="buttons" style="text-align: center; margin-top: 20px; margin-bottom: 20px;">     <a href="https://arxiv.org/abs/2406.13564" class="btn btn-lg z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper (ArXiv)</a>     <a href="[LINK_TO_YOUR_POSTER_FILE]" class="btn btn-lg z-depth-0" role="button">Poster</a>     <a href="https://github.com/kreimanlab/HumorDB" class="btn btn-lg z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code &amp; Dataset</a> </div> <hr> <h3 id="the-core-idea-what-does-it-mean-to-understand-an-image">The Core Idea: What does it mean to “understand” an image?</h3> <p>Modern AI can easily label the objects in the images below: “surgeon,” “patient,” “operating room”. But can it tell you <em>why</em> the image on the left is funny, while the nearly identical image on the right is not?</p> <p>This is the central question of HumorDB. We find that while AI is good at literal-level <em>classification</em>, it fails at human-level <em>abstract reasoning</em>. Humor, which relies on understanding context, expectations, and incongruity, is the perfect testbed for this.</p> <p>We introduce <strong>HumorDB</strong>, a new dataset and benchmark built on <strong>minimally contrastive pairs</strong>. We take a humorous image and make a subtle edit to remove <em>only</em> the humorous element, forcing the model to prove it can pinpoint the exact source of the joke.</p> <div class="row" style="margin-top: 20px; margin-bottom: 20px;">     <div class="col-sm mt-3 mt-md-0" style="text-align: center;">         <img class="img-fluid rounded z-depth-1" src="humordb_key_idea_1.png" alt="Funny surgeon image"> <p><strong>Funny</strong> (83% of humans agree)</p>     </div>     <div class="col-sm mt-3 mt-md-0" style="text-align: center;">         <img class="img-fluid rounded z-depth-1" src="humordb_key_idea_2.png" alt="Non-funny surgeon image"> <p><strong>Not Funny</strong> (86% of humans agree) </p>     </div> </div> <hr> <h3 id="key-findings-at-a-glance">Key Findings at a Glance</h3> <p>We tested state-of-the-art vision models (ViT, DINOv2) and large vision-language models (GPT-4o, Gemini-Flash) against human performance on three tasks: binary classification, funniness rating, and pairwise comparison (which image is funnier).</p> <div class="row" style="margin-top: 20px;"> <div class="col-sm mt-3 mt-md-0"> <strong>1. A Clear Human-AI Gap Remains</strong> <br> <p>Models perform well above chance, but all trail human-level accuracy. The gap is most pronounced in the "Comparison Task," which requires nuanced judgment.</p> <img class="img-fluid rounded z-depth-1" src="comparison_task_graph.png" alt="Graph showing AI performance below human performance"> </div> <div class="col-sm mt-3 mt-md-0"> <strong>2. AI Fails to "Look at the Joke"</strong> <br> <p>Even when a model is correct, its internal attention maps rarely focus on the humorous region. Models are "right for the wrong reason," relying on superficial cues, not the joke itself.</p> <img class="img-fluid rounded z-depth-1" src="attention_map_example.png" alt="Attention map showing AI looking at the wrong part of an image"> </div> <div class="col-sm mt-3 mt-md-0"> <strong>3. Abstraction is the Hardest Challenge</strong> <br> <p>Performance varied by image type. All models struggled most with abstract content like sketches, performing near chance-level.</p> <img class="img-fluid rounded z-depth-1" src="sketches_graph.png" alt="Graph showing performance on sketches is lowest"> </div> </div> <hr> <h3 id="the-dataset">The Dataset</h3> <p>HumorDB is a diverse, controlled dataset designed for rigorous evaluation.</p> <ul> <li> <strong>3,542</strong> Total Images</li> <li> <strong>1,271</strong> Minimally Contrastive Pairs</li> <li> <strong>650</strong> Human Subjects Annotating</li> <li> <strong>5 Image Types:</strong> Photos (36%), Photoshopped (35%), Cartoons (14%), Sketches (5%), and AI-Generated (10%)</li> </ul> <h4 id="examples-from-humordb">Examples from HumorDB</h4> <div class="row">     <div class="col-sm mt-3 mt-md-0">         <img class="img-fluid rounded z-depth-1" src="example_magic_carpet.png" alt="Optical illusion of a magic carpet"> <p style="color: red;"><i>GPT-4o: "...the optical illusion created by the shadow...makes it appear as though someone is flying on a magic carpet."</i></p>     </div>     <div class="col-sm mt-3 mt-md-0">         <img class="img-fluid rounded z-depth-1" src="example_dog_sunglasses.png" alt="Dog with sunglasses drinking from a coconut"> <p style="color: green;"><i>Gemini-Flash: "...the dog is wearing sunglasses and enjoying a coconut drink."</i></p>     </div>     <div class="col-sm mt-3 mt-md-0">         <img class="img-fluid rounded z-depth-1" src="example_person_sharpner.png" alt="Forced perspective of snowboarder on the moon"> <p style="color: red;"><i>Llava: "...depicts a cartoon of a person inside a box, seemingly being ”pulled out” by a hand using a toothpick. ..."</i> </p>     </div> </div> <hr> <h3 id="abstract">Abstract</h3> <blockquote> <p>Despite significant advancements in image segmentation and object detection, understanding complex scenes remains a significant challenge. Here, we focus on graphical humor as a paradigmatic example of image interpretation that requires elucidating the interaction of different scene elements in the context of prior cognitive knowledge. This paper introduces HumorDB, a novel, controlled, and carefully curated dataset designed to evaluate and advance visual humor understanding by AI systems. The dataset comprises diverse images spanning photos, cartoons, sketches, and AI-generated content, including minimally contrastive pairs where subtle edits differentiate between humorous and non-humorous versions. We evaluate humans, state-of-the-art vision models, and large vision-language models on three tasks: binary humor classification, funniness rating prediction, and pairwise humor comparison. The results reveal a gap between current AI systems and human-level humor understanding. While pretrained vision-language models perform better than vision-only models, they still struggle with abstract sketches and subtle humor cues. Analysis of attention maps shows that even when models correctly classify humorous images, they often fail to focus on the precise regions that make the image funny. Preliminary mechanistic interpretability studies and evaluation of model explanations provide initial insights into how different architectures process humor. Our results identify promising trends and current limitations, suggesting that an effective understanding of visual humor requires sophisticated architectures capable of detecting subtle contextual features and bridging the gap between visual perception and abstract reasoning. All the code and data are available here: https://github.com/kreimanlab/HumorDB.</p> </blockquote> <hr> <h3 id="citation">Citation</h3> <p>If you find our work useful, please consider citing:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">jain2025humordbaiunderstandgraphical</span><span class="p">,</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err">title={HumorDB:</span> <span class="err">Can</span> <span class="err">AI</span> <span class="err">understand</span> <span class="err">graphical</span> <span class="err">humor?</span><span class="p">}</span><span class="c">, </span>
<span class="c">      author={Vedaant V Jain and Felipe dos Santos Alves Feitosa and Gabriel Kreiman},</span>
<span class="c">      year={2025},</span>
<span class="c">      eprint={2406.13564},</span>
<span class="c">      archivePrefix={arXiv},</span>
<span class="c">      primaryClass={cs.CV},</span>
<span class="c">      url={[https://arxiv.org/abs/2406.13564](https://arxiv.org/abs/2406.13564)}, </span>
<span class="c">}</span>
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Vedaant Jain. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>